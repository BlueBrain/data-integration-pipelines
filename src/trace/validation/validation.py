"""
Data quality checks on ExperimentalTrace-s
Used for curation annotating purposes (see add_curation_annotation.py),
as well as batch reporting on the quality of the data (through and xlsx file generated by this script)

"""
import asyncio
from multiprocessing import Pool

import json
import os
from collections import defaultdict
import pandas as pd
from typing import Optional, List, Tuple, Dict, Union
from kgforge.core import Resource, KnowledgeGraphForge

from contextlib import redirect_stdout
import io

from src.logger import logger

from src.helpers import _as_list, Deployment, allocate_with_default_views, authenticate_from_parser_arguments

from src.trace.query.query import query_traces
from src.forge_extension import _retrieve_file_metadata, _exists
from src.trace.arguments import trace_command_line_args
# from src.trace.thumbnail import data_from_content_url
# from src.get_projects import get_all_projects

from src.trace.types_and_schemas import (
    EXPERIMENTAL_TRACE_SCHEMA, EXPERIMENTAL_TRACE_TYPE, TRACE_WEB_DATA_CONTAINER_TYPE
)

stimulus_type_exists_cache = {}


def exists_wrapper(
        id_: Union[str, List[str]], forge: KnowledgeGraphForge, operation_str: str, is_file: bool
) -> Union[bool, List[bool]]:

    f = io.StringIO()
    with redirect_stdout(f):
        res = _exists(provided_id=id_, forge=forge, is_file=is_file)

    if isinstance(id_, list):
        # res = _as_list(res)
        failed = [i for i, v in enumerate(res) if not v.succeeded]
        if len(failed) > 0:
            logger.warning(f"{operation_str} failed. {[id_[i] for i in failed]} do not exist")

        return [res_i.succeeded for res_i in res]

    if isinstance(id_, str):
        if not res.succeeded:
            logger.warning(f"{operation_str} failed. {id_} does not exist")

        return res.succeeded

    raise Exception(f"Unsupported type for id_ {type(id_)}")


def retrieve_wrapper(
        id_: Union[str, List[str]], forge: KnowledgeGraphForge, operation_str: str, is_file: bool
) -> Union[Optional[List[Resource]], Optional[Resource]]:

    f = io.StringIO()
    with redirect_stdout(f):
        res = forge.retrieve(id_) if not is_file else _retrieve_file_metadata(id_, forge)

    if isinstance(id_, list):
        res = _as_list(res)
        failed = [i for i, v in enumerate(res) if v is None]
        if len(failed) > 0:
            logger.warning(f"{operation_str} failed. Could not retrieve {[id_[i] for i in failed]}")

    if isinstance(id_, str):
        if res is None:
            logger.warning(f"{operation_str} failed. Could not retrieve {id_}")

    return res


def validate_wrapper(resource: Resource, forge: KnowledgeGraphForge, type_: str) -> bool:
    f = io.StringIO()
    with redirect_stdout(f):
        forge.validate(resource, type_=type_)
    return resource._last_action.succeeded


def complete_minds_metadata(resource: Resource, forge: KnowledgeGraphForge) -> bool:
    return validate_wrapper(resource, forge, type_="Dataset")


# 21.997    0.440
# 82.875    1.658 -> + 2 calls to validate, which adds 0.644 x 2 -> Too long
# TODO maybe parallelize validate outside of the thing...


def has_distribution(resource: Resource) -> bool:
    return "distribution" in resource.__dict__


def distribution_extension_from_name(resource: Resource, extension: str) -> Tuple[bool, bool, bool, str]:
    distribution = next((d for d in _as_list(resource.distribution) if d.name.split(".")[-1] == extension), None)

    has_distribution_ext = distribution is not None

    if has_distribution_ext:
        right_encoding_format = distribution.encodingFormat.split('/')[-1] == extension
        files_app = resource._store_metadata._project.replace("projects", "files")

        if isinstance(distribution.contentUrl, str):
            cu = distribution.contentUrl
        elif isinstance(distribution.contentUrl, Resource):
            cu = distribution.contentUrl.get_identifier()
        else:
            raise Exception(f"Weird distribution.contentUrl of type {type(distribution.contentUrl)} for {resource.get_identifier()}")

        distribution_is_self_like = files_app in cu
    else:
        right_encoding_format = False
        distribution_is_self_like = False
        cu = None

    return has_distribution_ext, right_encoding_format, distribution_is_self_like, cu


def has_image_property(resource: Resource) -> bool:
    return "image" in resource.__dict__


def has_stimulus_property(resource: Resource) -> bool:
    return "stimulus" in resource.__dict__


def images_in_same_bucket(resource: Resource, forge: KnowledgeGraphForge) -> bool:
    return all(
        exists_wrapper(
            i.get_identifier(),
            forge,
            f"Retrieve image-s of {resource.get_identifier()}",
            is_file=True
        ) for i in _as_list(resource.image)
    )


def stimuli_types_exists_in_ontology(
        list_to_check: List[Resource], forge_datamodels: KnowledgeGraphForge, extra_str: str
) -> Tuple[bool, bool, List]:

    stimuli_type = [
        (node.stimulusType.get_identifier() if "stimulusType" in node.__dict__ else None)
        for node in list_to_check
    ]
    all_have_stimuli_type = all(isinstance(i, str) for i in stimuli_type)

    if not all_have_stimuli_type:
        return False, False, stimuli_type

    for stimulus_type_id in stimuli_type:
        if stimulus_type_id not in stimulus_type_exists_cache:
            stimulus_type_exists_cache[stimulus_type_id] = exists_wrapper(stimulus_type_id, forge_datamodels, f"Retrieve stimulus type {extra_str}", is_file=False)

    stimuli_type_exist = all(stimulus_type_exists_cache[stimulus_type_id] for stimulus_type_id in stimuli_type)

    return all_have_stimuli_type, stimuli_type_exist, stimuli_type


def has_has_part_property(resource: Resource) -> bool:
    return "hasPart" in resource.__dict__


def has_part_is_part(resource: Resource, trace_web: Resource) -> bool:
    if "isPartOf" not in trace_web.__dict__:
        return False

    is_part_of = _as_list(trace_web.isPartOf)
    if len(is_part_of) > 1:
        logger.warning(f"more than one isPartOf for {trace_web.get_identifier()}")

    is_part_of_identifier = is_part_of[0].get_identifier()

    return is_part_of_identifier == resource.get_identifier()


def has_description(resource: Resource):
    return "description" in resource.__dict__


def e_type_getter(resource: Resource) -> Optional[str]:
    if "annotation" not in resource.__dict__:
        return None
    v = next(
        (
            i for i in _as_list(resource.annotation)
            # if "ETypeAnnotation" in _as_list(i.get_type()) # issues with this type value in some places like public/sscx
            if "EType" in _as_list(_as_list(i.hasBody)[0].get_type())
        ),
        None
    )

    if v is None:
        return None

    return v.hasBody.label


def trace_quality_check(
        resource: Union[str, Resource], forge: KnowledgeGraphForge, forge_datamodels: KnowledgeGraphForge,
        bool_only: bool = False
) -> Dict[str, Union[str, bool]]:
    """

    :param resource: the trace Resource to run the quality checks against
    :type resource: Resource
    :param forge: an instance of forge tied to the bucket where the resource belongs
    :type forge: KnowledgeGraphForge
    :param forge_datamodels: an instance of forge tied to neurosciencegraph/datamodels to check the stimulus type ontology
    :type forge_datamodels: KnowledgeGraphForge
    :param bool_only: Whether to only return checks that are bool-valued or not (exclude or include textual information)
    :type bool_only: KnowledgeGraphForge
    :return: the quality checks
    :rtype: Dict[str, Union[str, bool]]
    """

    if isinstance(resource, str):
        resource = forge.retrieve(resource)

    if resource is None:
        raise Exception(f"Couldn't retrieve resource {resource}")

    has_distribution_bool = has_distribution(resource)

    has_description_bool = has_description(resource)

    e_type = e_type_getter(resource)
    has_e_type = e_type is not None

    distribution_nwb_bool, nwb_encoding_format_bool, distribution_is_self_bool, nwb_content_url = \
        distribution_extension_from_name(resource, "nwb") if has_distribution_bool else (False, False)

    # thumbnail_can_be_generated = (data_from_content_url(content_url=nwb_content_url, token=forge._store.token) is not None) \
    #     if (has_distribution_bool and nwb_content_url is not None) else False

    has_image_property_bool = has_image_property(resource)

    has_image_in_same_bucket = images_in_same_bucket(resource, forge) if has_image_property_bool else False

    image_all_have_stimuli_type, image_stimuli_type_can_be_retrieved_from_ontology, image_stimulus_type_list = \
        stimuli_types_exists_in_ontology(
            _as_list(resource.image), forge_datamodels, extra_str=f"from image of {resource.get_identifier()}"
        ) if has_image_property_bool else (False, False, False)

    has_stimulus_property_bool = has_stimulus_property(resource)

    stimulus_all_have_stimuli_type, stimulus_stimuli_type_can_be_retrieved_from_ontology, stimulus_stimulus_type_list = \
        stimuli_types_exists_in_ontology(
            _as_list(resource.stimulus), forge_datamodels, extra_str=f"from stimulus of {resource.get_identifier()}"
        ) if has_stimulus_property_bool else (False, False, False)

    if has_stimulus_property_bool and has_image_property_bool:
        matching_list = set(image_stimulus_type_list) == set(stimulus_stimulus_type_list)
    else:
        matching_list = False

    (
        retrievable_has_part_bool,
        has_part_type_TraceWebDataContainer_bool,
        trace_web_data_container_has_distribution_bool,
        trace_web_data_container_has_distribution_rab_bool,
        rab_encoding_format_bool,
        trace_web_data_container_points_to_main_bool,
        trace_web_data_container_complete_minds_bool,
        rab_distribution_is_self_like,
        trace_web_data_container_schema,
        trace_web_data_container_has_right_schema,
        trace_web_validates_bool
    ) = (False, False, False, False, False, False, False, False, '', False, False)

    has_has_part_property_bool = has_has_part_property(resource)

    complete_minds_bool = complete_minds_metadata(resource, forge)

    if has_has_part_property_bool:

        has_part_retrieved = [
            retrieve_wrapper(
                i.get_identifier(),
                forge, f"Retrieve hasPart-s of {resource.get_identifier()}",
                is_file=False
            ) for i in _as_list(resource.hasPart)
        ]

        retrievable_has_part_bool = all(isinstance(i, Resource) for i in has_part_retrieved)

        trace_web_data_container_resource = \
            next(
                (
                    i for i in has_part_retrieved if isinstance(i, Resource) and TRACE_WEB_DATA_CONTAINER_TYPE in
                        [forge._model.context().expand(type_i) for type_i in _as_list(i.get_type())]
                ), None
            )

        has_part_type_TraceWebDataContainer_bool = isinstance(trace_web_data_container_resource, Resource)

        if has_part_type_TraceWebDataContainer_bool:
            # trace_web_data_container_schema = trace_web_data_container_resource._store_metadata._constrainedBy
            # trace_web_data_container_has_right_schema = trace_web_data_container_schema == TRACE_WEB_DATA_CONTAINER_SCHEMA
            # trace_web_validates_bool = validate_wrapper(resource=trace_web_data_container_resource, type_="TraceWebDataContainer", forge=forge)

            trace_web_data_container_has_distribution_bool = has_distribution(trace_web_data_container_resource)

            trace_web_data_container_has_distribution_rab_bool, rab_encoding_format_bool, rab_distribution_is_self_like, rab_content_url = \
                distribution_extension_from_name(trace_web_data_container_resource, "rab") \
                if trace_web_data_container_has_distribution_bool else (False, False)

            trace_web_data_container_points_to_main_bool = has_part_is_part(resource, trace_web_data_container_resource)

            # trace_web_data_container_complete_minds_bool = complete_minds_metadata(trace_web_data_container_resource, forge)

    resource_schema = resource._store_metadata._constrainedBy
    resource_self = resource._store_metadata._self
    trace_has_right_schema = resource_schema == EXPERIMENTAL_TRACE_SCHEMA
    trace_validates_bool = validate_wrapper(resource=resource, type_=EXPERIMENTAL_TRACE_TYPE, forge=forge)

    flags = {
        "Trace Identifier": resource.get_identifier(),
        "Trace Self": resource_self,
        "Trace Schema": resource_schema,

        "Trace has intended Schema": trace_has_right_schema,
        "Trace validates against intended schema": trace_validates_bool,
        "Trace has Complete MINDS": complete_minds_bool,
        "Trace has Distribution property": has_distribution_bool,

        "Trace Distribution is nwb from name": distribution_nwb_bool,
        "Trace has Description property": has_description_bool,

        # "Trace Distribution has nwb encoding format": nwb_encoding_format_bool,

        "Trace Distribution is self/self-like": distribution_is_self_bool,
        # "Trace Thumbnail can be generated": thumbnail_can_be_generated,
        "Trace Has Image property": has_image_property_bool,
        "Trace Image Can be Retrieved": has_image_in_same_bucket,
        "Trace Image all have stimulus type": image_all_have_stimuli_type,
        "Image stimulus type can be retrieved from ontology": image_stimuli_type_can_be_retrieved_from_ontology,
        "Trace has Stimulus property": has_stimulus_property_bool,
        "Trace Stimulus all have stimulus type": stimulus_all_have_stimuli_type,
        "Trace Stimulus stimuli type can be retrieved from ontology": stimulus_stimuli_type_can_be_retrieved_from_ontology,
        "Trace Stimulus and Image have matching stimulus type list": matching_list,
        "Trace has EType": has_e_type,
        "Trace EType value": e_type,
        "Trace has HasPart property": has_has_part_property_bool,
        "Trace HasPart can be Retrieved": retrievable_has_part_bool,
        "Trace HasPart of type TraceWebDataContainer": has_part_type_TraceWebDataContainer_bool,
        "TraceWebDataContainer has distribution": trace_web_data_container_has_distribution_bool,

        # "TraceWebDataContainer Schema": trace_web_data_container_schema,
        # "TraceWebDataContainer has intended schema": trace_web_data_container_has_right_schema,
        # "TraceWebDataContainer validates against intended schema": trace_web_validates_bool,

        "TraceWebDataContainer has rab distribution from name": trace_web_data_container_has_distribution_rab_bool,
        "TraceWebDataContainer Distribution is self/self-like": rab_distribution_is_self_like,

        # "TraceWebDataContainer has rab encoding format": rab_encoding_format_bool,

        "TraceWebDataContainer has isPartOf to Trace": trace_web_data_container_points_to_main_bool,

        # "TraceWebDataContainer Complete Minds": trace_web_data_container_complete_minds_bool
    }

    if bool_only:
        for keys in ["Trace EType value", "Trace Identifier", "Trace Self", "Trace Schema"]:
            del flags[keys]

    return flags


def single_resource_catch(
        id_: str, forge: KnowledgeGraphForge, forge_datamodels: KnowledgeGraphForge
) -> Tuple[Optional[Dict], Optional[str]]:

    try:
        return trace_quality_check(resource=id_, forge=forge, forge_datamodels=forge_datamodels), None
    except Exception as e:
        return None, f"Error for resource {id_}: {e}"


def stringify(v: Union[str, bool]) -> str:
    return v if not isinstance(v, bool) else ('' if v else 'False')


async def here_we_go(
        output_dir: str, deployment: Deployment, token: str, org: str, project: str, curated: str,
        excel_file_name="traces", error_filename="error_traces"
):
    forge_datamodels_instance = allocate_with_default_views(
        "neurosciencegraph", "datamodels", deployment=deployment, token=token
    )

    forge_instance = allocate_with_default_views(
        org, project, deployment=deployment, token=token
    )

    extra_q = """
      FILTER %s EXISTS {
        ?id nsg:annotation / nsg:hasBody <https://neuroshapes.org/Curated> .
      }
    """ % ("" if curated == "yes" else "NOT") if curated != "both" else ""

    trace_ids = query_traces(forge_instance, extra_q=extra_q)

    if trace_ids is None or len(trace_ids) == 0:
        logger.info(f"No ExperimentalTrace-s found in {org}/{project} (curated: {curated})")
        return

    logger.info(f"Found {len(trace_ids)} in {org}/{project} (curated: {curated})")

    res = Pool().starmap(
        single_resource_catch,
        [(t_id, forge_instance, forge_datamodels_instance) for t_id in trace_ids]
    )

    errors = defaultdict(list)

    writer = pd.ExcelWriter(
        path=os.path.join(output_dir, f'{excel_file_name}.xlsx'),
        engine='xlsxwriter',
        engine_kwargs={'options': {'strings_to_urls': False}}
    )

    rows = []
    bucket_str = f'{org}_{project}'

    for row, err_msg in res:
        if row is not None:
            rows.append(dict((k, stringify(v)) for k, v in row.items()))

        if err_msg is not None:
            logger.error(err_msg)
            errors[bucket_str].append(err_msg)

    df = pd.DataFrame(rows)

    bucket_str = f'{org}_{project}'

    bucket_str = bucket_str[:30] if len(bucket_str) > 31 else bucket_str

    df.to_excel(writer, index=False, sheet_name=bucket_str)

    worksheet = writer.sheets[bucket_str]

    for i, col in enumerate(df.columns):
        width = max(df[col].apply(lambda x: len(str(x))).max(), len(col))
        worksheet.set_column(i, i, width)

    writer.close()

    with open(os.path.join(output_dir, f"{error_filename}.json"), "w") as f:
        json.dump(errors, f, indent=4)


if __name__ == "__main__":

    parser = trace_command_line_args(with_bucket=True, with_curated=True)

    received_args, leftovers = parser.parse_known_args()

    deployment, auth_token = authenticate_from_parser_arguments(received_args)

    curated = received_args.curated
    org, project = received_args.bucket.split("/")

    output_dir = received_args.output_dir
    os.makedirs(output_dir, exist_ok=True)

    asyncio.run(
        here_we_go(
            output_dir=output_dir, deployment=deployment, token=auth_token,
            curated=curated, org=org, project=project
        )
    )
